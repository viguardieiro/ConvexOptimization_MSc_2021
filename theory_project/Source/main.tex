\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}

%\documentclass[twoside,final]{siamltex}
%\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[dvips]{graphicx}
\usepackage{color}
\usepackage{float} 
\usepackage{fullpage}
\usepackage{epsfig} 
\usepackage{amsthm}
\usepackage{amssymb}

\usepackage{algorithm2e}
\usepackage{algorithmic}

\usepackage[numbib]{tocbibind}

\usepackage{url}

\newcommand{\X}{\mathcal{X}}
\newcommand{\R}{\mathbb{R}}

\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{theorem}[thm]{Theorem}
\newtheorem{obs}[thm]{Observation}


\title{Notes - Interior Point Polynomial Time Methods for Linear Programming, Conic Quadratic Programming, and Semidefinite Programming}
\author{Vitoria Guardieiro}
\date{December 2021}

\begin{document}
\maketitle

\section{Motivation}

Generic convex problems, under mild computability and boundedness assumptions, are polynomially solvable, e.g., by the ellipsoid method. It turns out that both the strong (universality) and the weak (poor practical performance) features of the ellipsoid method have the same origin — the fact that the method deals with black box–represented problems. When starting the solution process, the method has no idea what problem it is working with. All it knows is how many design variables are in the problem and how large and thin its feasible set can be.

In practice, there are no entities like “a convex objective $f$” and “a convex feasible set $\X$”, with all our a priori knowledge of $f$ and $\X$ being expressed by the word “convex”. Normally we deal with instances of a known-in-advance generic convex program, like LP, CQP, or SDP, with known data and therefore we possess from the very beginning complete global information on the problem to be processed. What we would like to have is an optimization technique able to utilize efficiently our global knowledge of the instance and thus able to generate a solution much faster than a nearly blind algorithm like the ellipsoid method. The major event in the recent history of convex optimization, called sometimes interior point revolution, was the invention of such smart techniques.

\section{The interior penalty scheme}

Consider a constrained convex optimization program. One can w.l.o.g. make its objective linear, moving, if necessary, the actual objective to the list of constraints. Thus, let the problem be

\begin{equation}\label{eq:c}
    \underset{x}{\min} \{c^T x : x \in \mathcal{X} \in \R ^n\},\tag{C}
\end{equation}

where $\mathcal{X}$ is a closed convex set, which we assume to possess a nonempty interior.

Let us choose a barrier (also called an interior penalty function) $F(x)$ for the feasible set $\mathcal{X}$, which is a function that is well defined, smooth, strongly convex on the interior of $\mathcal{X}$ and that blows up as a point from $\text{int}\mathcal{X}$ approaches a boundary point of $\mathcal{X}$:

\begin{equation*}
    x_i \in \text{int} \mathcal{X}, x \equiv \underset{i\to \infty}{\lim} x_i \in \partial\mathcal{X} \implies \underset{i \to \infty}{\lim} F(x_i) = \infty.
\end{equation*}

We next consider the one-parametric family of functions generated by our objective and the barrier:

\begin{equation*}
    F_t(x) = tc^T x+F(x) : \text{int} \mathcal{X} \to \R
\end{equation*}

Here $t$ is the penalty parameter, which is assumed to be nonnegative.

Under mild regularity assumptions (e.g., in the case of bounded $\X$, which we assume from now on), we have that:

\begin{itemize}
    \item Every function $F_t(\cdot)$ attains its minimum over the interior of $\X$, the minimizer $x_*(t)$ being unique.
    \item The central path $x_*(t)$ is a smooth curve, and all its limiting, $t \to \infty$, points belong to the set of optimal solutions of (\ref{eq:c}).
\end{itemize}

To minimize $F_t(\cdot)$ for large $t$ is the same as minimizing the function $f_\rho (x) = f(x)+\rho F(x)$ for small $\rho =1/t$. When $\rho$ is small, the function $f_\rho$ is very close to $f$ everywhere in $\mathcal{X}$ except a narrow stripe along the boundary of $\mathcal{X}$, the stripe becoming thinner and thinner as $\rho \to 0$. Therefore we have good reasons to believe that the minimizer of $F_t$ for large $t$ (i.e., the minimizer of $f_\rho$ for small $\rho$) must be close to the set of minimizers of $f$ on $\X$. The central path $x_*(t)$ is a kind of Ariadne's thread that leads to the solution set of (\ref{eq:c}).

Given a value $t \geq 0$ of the penalty parameter, to reach the point $x_*(t)$ on this path is the same as minimizing a smooth strongly convex function $F_t(\cdot)$ which attains its minimum at an interior point of $\mathcal{X}$. The latter problem is nearly unconstrained, up to the fact that its objective is not everywhere defined. However, we can easily adapt the methods of unconstrained minimization, including the Newton one, to handle nearly unconstrained problems. We see that constrained convex optimization in a sense can be
reduced to the easy unconstrained one.

Assume that we know in advance the minimizer of $F_0 \equiv F$, i.e., the point $x_*(0)$. Thus, we know where the central path starts. Now let us follow this path: at $i$th step, standing at a point $x_i$ close enough to some point $x_*(t_i)$ of the path, we
\begin{itemize}
\item first, increase a bit the current value $t_i$ of the penalty parameter, thus getting a new target point $x_*(t_{i+1})$ on the path; and
\item second, approach our new target point $x_*(t_{i+1})$ by running, say, the Newton method on the function $F_{t_{t+1}}$ and starting the method at our current iterate $x_i$, until a new iterate $x_{i+1}$ close enough to $x_*(t_{i+1})$ is generated.
\end{itemize}

Iterating this updating and strengthening appropriately our closeness requirements as the process goes on, we approach the optimal set along the central path. If all our requirements of “close enough” and “not too rapidly” are properly controlled, we may ensure $x_i$ to be in the domain of quadratic convergence of the Newton method as applied to $F_{t_{i+1}}$, which means that it will take a quite small number of steps to recover closeness to our new target point.

However, as far as complexity is concerned, for nearly all black box–represented classes of unconstrained convex optimization problems, there is no such phenomenon as local quadratic convergence, the Newton method (which uses the second order derivatives) has no advantages over the methods that use only first order derivatives.

What prevents this scheme from yielding a polynomial time method is not the structure of the scheme but the complete freedom it allows for some of its elements. After some order is added, the scheme becomes a polynomial
time one. It turns out that we can do that for all interesting well-structured generic convex programs, in particular, for linear, conic quadratic, and semidefinite programming.

\section{Interior point methods for linear programming, conic quadratic programming, and semidefinite programming: Building blocks}

\subsection{Canonical cones and canonical barriers}

We will be interested in a generic conic problem

\begin{equation}
    \underset{x}{\min} \{c^Tx : \mathcal{A}x-B \in \textbf{K}\} \tag{CP}
\end{equation}

associated with a cone $\textbf{K}$ given as a direct product of m basic cones, each being either a Lorentz ($\textbf{L}$) or a semidefinite cone ($\textbf{S}_{+}$):

\begin{equation*}\label{eq:cone}
    \textbf{K} = \textbf{S}_+^{k_1}\times\dots\times\textbf{S}_+^{k_p}\times\textbf{L}^{k_{p+1}}\times\dots\times\textbf{L}^{k_{m}} \tag{Cone}
\end{equation*}

The generic problem in question covers LP (no Lorentz cones in the right-hand side, and all semidefinite cones are of dimension 1), CQP (no semidefinite cones), and SDP (no Lorentz cones).

We equip the semidefinite and the Lorentz cones with canonical barriers:

\begin{itemize}
    \item Canonical barrier for a semidefinite cone $\textbf{S}_+^k$: 
    \begin{equation*}
        S_k(X) = -\ln \text{Det}(X) : \text{int} \textbf{S}_+^k \to \R;
    \end{equation*}
the parameter of this barrier, by definition, is $\theta(S_k)=k$.
    \item Canonical barrier for a Lorentz cone $\textbf{L}^k$:
    \begin{equation*}
        L_k(x) = -\ln(x_k^2-x_1^2-\dots -x_{k-1}^2);
    \end{equation*}
    with parameter $\theta(L_k) = 2$.
    \item Canonical barrier for the cone $\textbf{K}$ given by (\ref{eq:cone}):
    \begin{equation*}
        K(X) = S_{k_1}(X_1)+\dots+S_{k_p}(X_p)+L_{k_{p+1}}(X_{p+1})+\dots+L_{k_{m}}(X_{m}),
    \end{equation*}
    \begin{equation*}
        X_i \in 
        \begin{cases}
          \text{int}\textbf{S}_+^{k_i}, & \ i\leq p \\
          \text{int}\textbf{L}^{k_i}, & \ p<i\leq m.
        \end{cases}
    \end{equation*}
    The parameter of the barrier $K(\cdot)$ is the sum of parameters of the basic barriers involved:
    \begin{equation*}
        \theta(K) = \theta(S_{k_1})+\dots+\theta(S_{k_p})+\theta(L_{k_{p+1}})+\dots+\theta(L_{k_{m}}) = \overset{p}{\underset{i=1}{\sum}} k_i + 2(m-p).
    \end{equation*}
\end{itemize}

We can see that our basic barriers (and therefore their direct sum $K(\cdot)$) are indeed barriers for the corresponding cones: they are $C^\infty$-smooth on the interiors of their domains, blow up to $\infty$ along every sequence of points from these interiors converging to a boundary point of the corresponding domain, and are strongly convex.

\subsection{Elementary properties of canonical barriers}

Let us establish a number of simple and useful properties of canonical barriers.

\begin{prop}
    A cononical barrier $F$ ($F$ can be $S_k$, $L_k$, or the direct sum $K$ of several copies of these elementary barriers) possesses the following properties:
    \begin{enumerate}
        \item[(i)] $F$ is logarithmically homogeneous with the parameter of logarithmic homogeneity equal to $-\theta(F)$, i.e., the following identity holds:
        \begin{equation*}
            t>0, x \in \text{Dom}F \implies F(tx) = F(x)-\theta(F)\ln t.
        \end{equation*}
        
        \item[(ii)] The following two equalities hold identically in $x \in \text{Dom}F$:
        \begin{enumerate}
            \item[(a)] $\langle\nabla F(x), x\rangle = -\theta(F)$,
            \item[(b)] $[\nabla ^2F(x)]x = -\nabla F(x)$.
        \end{enumerate}
        
        \item[(iii)] The $l^\text{th}$ differential $D^l F(x)$ of $F$, $l\geq 1$, is homogeneous of degree $-l$ in $x \in \text{Dom}F$:

    $\forall(x \in \text{Dom}F, t>0, h_1, \dots, h_l):$
    
    \begin{equation}
        D^l F(tx)[h_1, \dots, h_l] \equiv \left.\frac{\partial^l F(tx+s_1h_1+\dots+s_lh_l)}{\partial s_1\partial s_2 \dots \partial s_l}\right|_{s_1=\dots=s_l=0} = t^{-l}D^lF(x)[h_1,\dots,h_l].
    \end{equation}
    \end{enumerate}
\end{prop}

\begin{proof}
(i) Both $S_k$ and $L_k$ are clearly are logarithmically homogeneous with the parameters of logarithmic homogeneity $-\theta(S_k)$, $-\theta(L_k)$, respectively. Also, the property of logarithmic homogeneity is stable with respect to taking direct sums of functions, so the direct sum $K$ is also logarithmically homogeneous.

(ii) To get (ii)(a), it suffices to differentiate the identity
\[F(tx) = F(x)-\theta(F)\ln t\]
in $t$ at $t=1$,
\[\langle\nabla F(tx), x\rangle = \frac{d}{dt}F(tx) = -\theta(F)t^{-1}\]
and it remains to set $t=1$ in the concluding identity.

Similarly, to get (ii)(b), it suffices to differentiate the identity
\[\langle\nabla F(x+th), x+th\rangle = -\theta(F)\]
(which is just (ii)(a)) in $t$ at $t = 0$, thus arriving at
\[\langle[\nabla^2 F(x)]h, x\rangle+\langle\nabla F(x), h\rangle = 0\]
since $\langle[\nabla^2 F(x)]h, x\rangle = \langle[\nabla^2 F(x)]x, h\rangle$ and since the resulting equality
\[\langle[\nabla^2 F(x)]x, h\rangle+\langle\nabla F(x), h\rangle = 0\]
holds true identically in $h$, we come to $[\nabla^2 F(x)]x = -\nabla F(x)$.

(iii) Differentiating $l$ times the identity
\[F(tx) = F(x)-\theta \ln t\]
in $x$, we get
\[t^l D^l F(tx)[h_1, \dots, h_l] = D^l F(x)[h_1,\dots, h_l].\]
\end{proof}


An especially nice specific feature of canonical barriers is their self-duality:

\begin{prop}\label{prop:dual}
    A canonical barrier $F$ possesses the following property: for every $x \in \text{Dom}F$, $-\nabla F(x)$ belongs to $\text{Dom}F$ as well, and the mapping $x \mapsto -\nabla F(x) : \text{Dom}F \to \text{Dom}F$ is self-inverse,
    \begin{equation}
        -\nabla F(-\nabla F(x)) = x \qquad \forall x \in \text{Dom},
    \end{equation}
    and is homogeneous of degree $-1$:
    \begin{equation}
        t>0, x \in \text{intdom}F \implies -\nabla F(tx) = -t^{-1} \nabla F(x).
    \end{equation}
\end{prop}

\section{Primal-dual pair of problems and the primal-dual central path}

\subsection{Problem(s)}

Since $\textbf{K}$ is a direct product of self-dual cones, this dual is a conic problem on the same cone $\textbf{K}$. As we remember, the primal-dual pair associated with (CP) is

\begin{equation}
    \underset{x}{\min} \{c^Tx : \mathcal{A}x-B \in \textbf{K}\} \tag{CP}
\end{equation}
\begin{equation}
    \underset{S}{\max} \{\langle B,S \rangle_E : \mathcal{A}*S = c, S \in \textbf{K}\} \tag{CD}
\end{equation}

Assuming that $\text{Null}(\mathcal{A}) = \{0\}$, we can write down our primal-dual pair in a symmetric geometric form:

\begin{equation}
    \underset{X}{\min} \{\langle C,X \rangle_E : X \in (\mathcal{L}-B) \cap \textbf{K}\} \tag{P}
\end{equation}
\begin{equation}
    \underset{S}{\max} \{\langle B,S \rangle_E : S \in (\mathcal{L}^\perp+B) \cap \textbf{K}\} \tag{D}
\end{equation}

where $\mathcal{L}$ is a linear subspace in $E$ (the image space of the linear mapping $x \mapsto \mathcal{A}x$), $\mathcal{L}^\perp$ is the orthogonal complement to $\mathcal{L}$ in $E$, and $C \in E$ satisfies $\mathcal{A}*C = c$, i.e., $\langle C,\mathcal{A}x \rangle_E \equiv c^Tx$.

To simplify things, from now on we assume that both problems (CP) and (CD) are strictly feasible. In terms of (P) and (D) this assumption means that both the primal feasible plane $\mathcal{L}-B$ and the dual feasible plane $\mathcal{L}^\perp+B$ intersect the interior of the cone $\textbf{K}$.

\subsection{Central path(s)}

The canonical barrier $K$ of $\textbf{K}$ induces a barrier for the feasible set $\X = \{x \ |\ \mathcal{A}x - B \in \textbf{K}\}$ of the problem (CP) written down in the form of (C), i.e., as

\begin{equation*}
    \underset{x}{\text{min}} \{c^Tx : x \in \X\};
\end{equation*}

this barrier is

\begin{equation*}
    \hat{K}(x) = K(\mathcal{A}x - B) : \text{int}X \to \R
\end{equation*}

and is indeed a barrier. Now we can apply the interior penalty scheme to trace the central path $x_*(t)$ associated with this barrier.

It can be derived from the primal-dual strict feasibility that the central path is well defined, i.e., that the minimizer of
\begin{equation*}
    \hat{K}_t(x)  = tc^Tx+\hat{K}(x)
\end{equation*}

on int$X$ exists for every $t \geq 0$ and is unique.

It is highly instructive to pass from the central path $x_*(t)$ in the space of design variables to its image in $E$:

\[X_*(t) = \mathcal{A}x_*(t)-B.\]

The resulting curve is called the \textit{primal central path} of the primal-dual pair (P), (D); by its origin, it is a curve comprising strictly feasible solutions of (P).

\begin{obs}\label{obs1}
A point $X_*(t)$ of the primal central path is the minimizer of the aggregate
\[P_t(X) = t\langle C, X\rangle_E + K(X)\]
on the set $(\mathcal{L}-B) \cap \text{int}\textbf{K}$ of strictly solutions of (P).
\end{obs}

\begin{obs}\label{obs2}
A point $X_*(t)$ of the primal central path is exactly the strictly feasible solution $X$ to (P) such that the vector $tC+\nabla K(X)\in E$ is orthogonal to $\mathcal{L}$ (i.e., belongs to $\mathcal{L}^\perp$).
\end{obs}

The dual problem (D) also possesses the central path, now called the \textit{dual central path} $S_*(t), t\geq 0$, of the primal-dual pair (P), (D). Similar to \ref{obs1} and \ref{obs2}, the dual central path can be characterized as follows:

\begin{obs}\label{obs3}
A point $S_*(t), t\geq 0$, of the dual central path is the unique minimizer of the aggregate
\[D_t(S) = -t\langle B,S\rangle_E+K(S)\]
on the set of strictly feasible solutions of (D). $S_*(t)$ is exactly the strictly feasible solution $S$ to (D) such that the vector $-tB+\nabla F(S)$ is orthogonal to $\mathcal{L}^\perp$ (i.e., belongs to $\mathcal{L}$).
\end{obs}

From Proposition \ref{prop:dual} we can derive a connection between the primal and the dual central paths:

\begin{theorem}
    For $t>0$, the primal and the dual central paths $X_*(t), S_*(t)$ of a (strictly feasible) primal-dual pair (P), (D) are linked by the relations
    \begin{align*}
        S_*(t) &= -t^{-1}\nabla K(X_*(t)),\\
        X_*(t) &= -t^{-1}\nabla K(S_*(t).
    \end{align*}
\end{theorem}

\begin{proof}
By Observation \ref{obs2}, the vector $tC+\nabla K(X_*(t))$ belongs to $\mathcal{L}^\perp$, so the vector $S = -t^{-1}\nabla K(X_*(t))$ belongs to the dual feasible plane $\mathcal{L}^\perp+C$. On the other hand, the vector $-\nabla K(X_*(t))$ belongs to Dom$K$, i.e., to the interior of $\textbf{K}$. Since $\textbf{K}$ is a cone and $t>0$, the vector $S=-t^{-1}\nabla K(X_*(t))$ belongs to the interior of $\textbf{K}$ as well. Thus, $S$ is a strictly feasible solution of (D). Now let us compute the gradient of the aggregate $D_t$ at the point $S$:
\begin{align*}
    \nabla D_t(S) 
    &= -tB+\nabla K(-t^{-1}\nabla K(X_*(t)))\\
    &= -tB+t\nabla K(-\nabla K(X_*(t)))\\
    &= -tB - tX_*(t)\\
    &= -t(B+X_*(t))\\
    &\in \mathcal{L}.
\end{align*}

Thus, $S$ is strictly feasible for (D) and $\nabla D_t(S) \in \mathcal{L}$. But by Observation \ref{obs3} these properties characterize $S_*(t)$; thus, $S_*(t)=S\equiv -t^{-1}\nabla K(X_*(t))$. In view of Proposition \ref{prop:dual}, this implies that $X_*(t) = -t^{-1}\nabla K(S_*(t))$. Another way to get the latter relation from the relation $S_*(t) = -t^{-1}\nabla K(X_*(t))$ is just to refer to the primal-dual symmetry.
\end{proof}

\begin{theorem}\label{teo:charac}
    Let (P), (D) be a strictly feasible primal-dual pair.
    
    For every $t>0$, there exists a unique strictly feasible solution $X$ of (P) such that $-t^{-1}\nabla K(X)$ is a feasible solution to (D), and this solution $X$ is exactly $X_*(t)$.
    
    Similarly, for every $t>0$, there exists a unique strictly feasible solution of (D) such that $-t^{-1}\nabla K(S)$ is a feasible solution of (P), and this solution $S$ is exactly $S_*(t)$.
\end{theorem}

\begin{proof}
By primal-dual symmetry, it suffices to prove the first claim. We know from the previous theorem that $X=X_*(t)$ is a strictly solution of (P) such that $-t^{-1}\nabla K(X)$ is feasible for (D). All we need to prove is that $X_*(t)$ is the only point with these properties, which is immediate: if $X$ is a strictly feasible solution of (P) such that $-t^{-1}\nabla K(X)$ is dual feasible, then $-t^{-1}\nabla K(X) \in \mathcal{L}^\perp+C$, whence $\nabla K(X)\in \mathcal{L}^\perp-tC$, or, which is the same, $\nabla P_t(X) = tC+\nabla K(X) \in \mathcal{L}^\perp$. We already know from Observation \ref{obs2} that the latter property, together with the strict primal feasibility, is characteristic for $X_*(t)$.
\end{proof}

\subsubsection{Characterization of the central path}

As we have seen, the primal and dual central paths are intrinsically linked one to another, and
it makes sense to treat them as a unique entity—the primal-dual central path of the primal-
dual pair (P), (D). From now on we refer to the primal-dual central path simply as the
\textit{central path}.

The points $(X_*(t), S_*(t))$ fo the central path possess the following properties:

\begin{itemize}
    \item \textbf{CP$_1$ (primal feasibility):} The point $X_*(t)$ is strictly primal feasible.
    \item \textbf{CP$_2$ (dual feasibility):} The point $S_*(t)$ is dual feasible.
    \item \textbf{CP$_3$ (augmented complementary slackness):} The points $X_*(t)$ and $S_*(t)$ are linked by the relation
    \[S_*(t) = -t^{-1}\nabla K(X_*(t)) \iff X_*(t) = -t^{-1}\nabla K(S_*(t))\]
\end{itemize}

Properties CP$_1$, CP$_2$, CP$_3$ fully characterize the central path: if two points
$X, S$ possess these properties with respect to some $t > 0$, then $X=X_*(t)$ and $S=S_*(t)$ (Theorem \ref{teo:charac}).

\subsubsection{Duality gap along the central path}

\begin{prop}
    Under assumption of primal-dual strict feasibility, the duality gap along the central path is inverse proportional to the penalty parameter, and the proportionality coefficient is the parameter $\theta(K)$ of the canonical barrier $K$:
    \[\text{DualityGap}(X_*(T), S_*(t)) = \frac{\theta(K)}{t}.\]
    
    In particular, both $X_*(t)$ and $S_*(t)$ are strictly feasible $(\theta(K)/t)$-approximate solutions to thei respective problems:
    \begin{align*}
        \langle C, X_*(t)\rangle_E -\text{Opt}(P) &\leq \frac{\theta(K)}{t},\\
        \text{Opt}(D)-\langle B, S_*(t)\rangle_E &\leq \frac{\theta(K)}{t}.
    \end{align*}
\end{prop}

\begin{proof}
\begin{align*}
    \text{DualityGap}(X_*(t), S_*(t))
    &= \langle S_*(t), X_*(t)\rangle_E\\
    &= \langle -t^{-1}\nabla K(X_*(t)), X_*(t)\rangle_E\\
    &= t^{-1}\theta(K)
\end{align*}
\end{proof}

\subsubsection{Distance to the central path}

The conclusion so far is that our life when moving along the central path would be just fine (at the very least, we would know how good the solutions we already have are). But \textit{how close should we be to the path (and in what sense close) for our life to be as nice as if we were exactly on the path?}

Our canonical barrier $K(\cdot)$ is a strongly convex smooth function on int$\textbf{K}$; in particular, its Hessian matrix $\nabla^2 K(Y)$, taken at a point $Y \in \text{int}\textbf{K}$, is positive definite. We can use the inverse of this matrix to measure the distances between points of $E$, thus arriving at the norm
\[||H||^*_Y = \sqrt{\langle[\nabla^2 K(Y)]^{-1}H,H \rangle_E}.\]

It turns out that a good measure of proximity of a strictly feasible primal-dual pair $Z = (X,S)$ to a point $Z_*(t) = (X_*(t), S_*(t))$ from the primal-dual central path is the quantity

\[\text{dist}(Z,Z_*(t)) \equiv ||tS+\nabla K(X)||^*_X \equiv \sqrt{\langle[\nabla^2 K(X)]^{-1}(tS+\nabla K(X)), tS+\nabla K(X) \rangle_E}\]

\begin{prop}
    If $Z=(X,S)$ is a pair of primal-dual strictly feasible solutions to (P), (D) such that
    \[dist(Z, Z_*(t)) \leq 1\]
    then $Z$ is essentially as good as $Z_*(t)$, namely, the duality gap as $(X,S)$ is essentially as small as at the point $Z_*(t)$:
    \[\text{DualityGap}(X,S) = \langle S,X\rangle_E \leq 2\text{DualityGap}(Z_*(t)) = \frac{2\theta(K)}{t}.\]
\end{prop}

It follows from this that for our purposes, it is essentially the same to move along the primal-dual central path or to trace this path, staying in its time-space neighborhood
\[\mathcal{N}_\kappa = \{(t,X,S) | X \in \mathcal{L}-B, S\in \mathcal{L}^\perp+C, t>0, \text{dist}((X,S),(X_*(t),S_*(t)))\leq \kappa\}\]
with certain $\kappa \leq 1$.

\section{Tracing the central path}

\subsection{Path-following scheme}

Assume we are solving a strictly feasible primal-dual pair of problems (P), (D) and intend to trace the associated central path. For this purpose, we just need a mechanism for updating a current iterate $(\Bar{t}, \Bar{X}, \Bar{S})$ such that $\Bar{t}<0$, $\Bar{X}$ is strictly primal feasible, $\Bar{S}$ is strictly dual feasible, and $(\Bar{X}, \Bar{S})$ is a good approximation of the point $Z_*(\Bar{t}) = (X_*(\Bar{t}), S_*(\Bar{t}))$ on the central path, into a new iterate $(t_+, X_+, S_+)$ with similar properties and a larger value $t_+ > \Bar{t}$ of the penalty parameter. Given such an updating mechanism and iterating it, we indeed shal trace the central path.

How could we construct the required updating? Recalling the description of the central path, we rephrase our question as:

\begin{quote}
\textit{Given a triple $(\Bar{t}, \Bar{X}, \Bar{S})$ that satisfies the relations
\begin{align}\label{sys:4}
    X \in \mathcal{L}-B,
    S \in \mathcal{L}^\perp+C
\end{align}
and approximately satisfies the system of nonlinear equations
\begin{equation} \label{eq:5}
    G_t(X,S) \equiv S+t^{-1}\nabla K(X) = 0,
\end{equation}
update it into a new triple $(t_+, X_+, S_+)$ with the same properties and such that $t_+>\Bar{t}$.
}
\end{quote}

The most natural way, from the viewpoint of computational mathematics, to achieve our target is as follows:
\begin{enumerate}
    \item Choose somehow a desired new value $T_+ > \Bar{t}$ of the penalty parameter.
    \item Linearize the left-hand-side $G_{t_+}(X,S)$ of the system of nonlinear equations at the point $(\Bar{X}, \Bar{S})$, and replace (\ref{eq:5}) with the linearized system of equations
    \begin{equation}\label{eq:6}
        G_{t_+}(\Bar{X},\Bar{S})+\frac{\partial G_{t_+}(\Bar{X},\Bar{S})}{\partial X}(X-\Bar{X})+\frac{\partial G_{t_+}(\Bar{X}, \Bar{S}}{\partial S}(S-\Bar{S}) = 0.
    \end{equation}
    \item Compute the corrections $\Delta X, \Delta S$ from the requirement that the updated pair $X_+ = \Bar{X}+\Delta X$, $S_+ = \Bar{S}+\Delta S$ must satisfy (\ref{sys:4}) and (\ref{eq:6}). In other words, the corrections should solve the system
    \begin{gather*}
        \Delta X \in \mathcal{L},\\
        \Delta S \in \mathcal{L}^\perp,\\
        G_{t_+}(\Bar{X},\Bar{S})+\frac{\partial G_{t_+}(\Bar{X},\Bar{S})}{\partial X}\Delta X+\frac{\partial G_{t_+}(\Bar{X}, \Bar{S}}{\partial S}\Delta S = 0
    \end{gather*}
    \item Complete updating the current solution by setting
    \begin{align*}
        X_+ &= \Bar{X}+\Delta X,\\
        S_+ &= \Bar{S}+\Delta S.
    \end{align*}
\end{enumerate}

The primal-dual interior point methods we are describing basically fit the outlined
scheme. However, if the current iterate $(\Bar{X},\Bar{S})$ is not close enough to $Z_*(\Bar{t})$, or the desired improvement $t_+-\Bar{t}$ is too large, the corrections given by the outlined scheme may be too large. As a result, the updating may be inappropriate. The standard way to overcome this difficulty is to replace the updating step with

\begin{align}
    X_+ &= \Bar{X}+\alpha \Delta X,\\
    S_+ &= \Bar{S}+\beta \Delta S,
\end{align}

and to choose the stepsizes $\alpha >0,\beta>0$ from additional safety considerations, like ensuring the updated pair $(X_+, S_+)$ to reside in the interior of $\textbf{K}$, or enforcing it to stay in a desired neighborhood of the central path, etc.

\subsection{Speed of path-tracing}

In the LP-CQP-SDP situation, the speed at which the best, from the theoretical viewpoint, path-following methods manage to trace the path is inversely proportional to the square root of the parameter $\theta(K)$ of the underlying canonical barrier. This means that when started at a point $(t^0, X^0, S^0)$ from the neighborhood $\mathcal{N}_{0.1}$ of the central path, the method after $O(1)\sqrt{\theta(K)}$ steps reaches the point $(t^1=2t^0, X^1, S^1) \in \mathcal{N}_{0.1}$, and so on.

Thus, it takes a fixed number $O(1)\sqrt{\theta(K)}$ steps to increase the current value of the penalty parameter by factor $2$, staying all the time in $\mathcal{N}_0.1$. It then follows that every $O(1)\sqrt{\theta(K)}$ steps of the method reduce the (upper bound on the) inaccuracy of current approximate solutions by factor $2$ or, which is the same, add a fixed number of accuracy digits to these solutions. Thus, the cost of an accuracy digit for the (best) path-following methods is $O(1)\sqrt{\theta(K)}$ steps.


\subsection{Primal and dual path-following methods}

The complexity analysis of the primal path-following method can be summarized in
the following theorem:

\begin{theorem}
    Let $0<\chi \leq \kappa\leq0.1$. Assume that we are given a starting point $(t_0, x_0, S_0)$ such that $t_0 > 0$ and the point 
    \[(X_0 = \mathcal{A}x_0-B, S_0)\]
    is $\kappa$-close to $Z_*(t_0)$:
    \[\text{dist}((X_0,S_0), Z_*(t_0)) \leq \kappa.\]
    Starting with $(t_0, x_0, X_0, S_0)$, let us build the iterates $(t_i, x_i, X_i, S_i)$ according to 
    \[t_i = \left(1+\frac{\chi}{\sqrt{\theta(K)}}\right)t_{i-1},\]
    \[x_i = x_{i-1}+\underbrace{(-[\mathcal{A}^*(\nabla^2K(X_{i-1}))\mathcal{A}]^{-1}[t_ic+\mathcal{A}^*\nabla K(X_{i-1})])}_{\Delta x_i},\]
    \[X_i = \mathcal{A}x_i - B,\]
    \[S_i = -t_i^{-1}[\nabla K(X_{i-1})+[\nabla^2 K(X_{i-1})\mathcal{A}\Delta x_i]].\]
    
    The resulting process is well defined and generates strictly primal-dual feasible pairs $(X_i, S_i)$ such that $(t_i, X_i, S_i)$ stay in the neighborhood $\mathcal{N}_k$ of the primal-dual central path.
\end{theorem}

The theorem says that, with properly chosen $\kappa, \chi$ (e.g., $\kappa = \chi=0.1)$, after we somehow reached the $\mathcal{N}_k$-neighborhood of the primal-dual central path, we can trace it by the primal path-following method, keeping the iterates in $\mathcal{N}_k$ and increasing the penalty parameter by an absolute constant factor in every $O(\sqrt{\theta(K)})$ steps.

\section{Complexity bounds for linear programming, conic quadratic programming, and semidefinite programming}



\subsection{Complexity of linear programming}

\textbf{Problem instance:}
\begin{equation}
    \underset{x \in \R^n}{\min} \{c^Tx : a_i^Tx \leq b_i, i=1,\dots, m; ||x||_\infty \leq R\}; \tag{$p$}
\end{equation}

\textbf{Data:}
\begin{align*}
    \text{Data}(p) &= [n;m;c;a_1,b_1;\dots;a_m,b_m;R],\\
    \text{Size}(p) &\equiv \dim \text{Data}(p) = (m+1)(n+1)+2.
\end{align*}

\textbf{$\epsilon$-solution:} an $x \in \R^n$ such that
\begin{align*}
    ||x||_\infty &\leq R,\\
    a_i^T &\leq b_i+\epsilon, i=1,\dots,m,\\
    c^Tx &\leq \text{Opt}(p)+\epsilon
\end{align*}

\textbf{Newton complexity of $\epsilon$-solution:}
\[\text{Compl}^\text{Nwt}(p\epsilon) = O(1)\sqrt{m+n}\text{Digits}(p,\epsilon),\]
where
\[\text{Digits}(p,\epsilon)=\ln\left(\frac{\text{Size}(p)+||\text{Data}(p)||_1+\epsilon^2}{\epsilon}\right)\]
is the number of accuracy digits in $\epsilon$-solution.

\textbf{Arithmetic complexity of $\epsilon$-solution:}
\[\text{Compl}(p,\epsilon) = O(1)(m+n)^{3/2}n^2\text{Digits}(p,\epsilon).\]

\subsection{Complexity of conic quadratic programming}

\textbf{Problem instance:}
\begin{equation}
    \underset{x \in \R^n}{\min} \{c^Tx : ||A_ix+b_i||_2 \leq c_i^Tx+d_i, i=1,\dots, m; ||x||_2 \leq R\};\tag{$p$}
\end{equation}

\textbf{Data:}
\begin{align*}
    \text{Data}(p) &= [n;m;k_1,\dots,k_m;c;A_1,b_1,c_1,d_1;\dots;A_m,b_m,c_m,d_m;R],\\
    \text{Size}(p) &\equiv \dim \text{Data}(p) = (m+\sum_{i=1}^{m}k_i)(n+1)+m+n+3.
\end{align*}

\textbf{$\epsilon$-solution:} an $x \in \R^n$ such that
\begin{align*}
    ||x||_2 &\leq R,\\
    ||A_ix+b_i||_2 &\leq c_i^Tx+d_i+\epsilon, i=1,\dots,m,\\
    c^Tx &\leq \text{Opt}(p)+\epsilon
\end{align*}

\textbf{Newton complexity of $\epsilon$-solution:}
\[\text{Compl}^\text{Nwt}(p\epsilon) = O(1)\sqrt{m+1}\text{Digits}(p,\epsilon).\]

\textbf{Arithmetic complexity of $\epsilon$-solution:}
\[\text{Compl}(p,\epsilon) = O(1)(m+n)^{1/2}n\left(n^2+m+\sum_{i=0}^m k_i^2\right)\text{Digits}(p,\epsilon).\]

\subsection{Semidefinite programming}

\textbf{Problem instance:}
\begin{equation}
    \underset{x \in \R^n}{\min} \left\{ c^Tx : A_0+\sum_{j=1}^n x_j A_j \succeq 0; ||x||_2 \leq R \right\},\tag{$p$}
\end{equation}
where $A_j, j=0,1,\dots,n$, are symmetric block-diagonal matrices with $m$ diagonal blocks $A_j^(i)$ of sizes $k_i\times k_i, i=1,\dots,m$.

\textbf{Data:}
\begin{align*}
    \text{Data}(p) &= [n;m;k_1,\dots,k_m;c;A_0^{(1)},\dots,A_0^{(m)};A_n^{(1)},\dots,A_n^{(m)};R],\\
    \text{Size}(p) &\equiv \dim \text{Data}(p) = \left(\sum_{i=1}^{m}\frac{k_i(k_i+1)}{2}\right)(n+1)+m+n+3.
\end{align*}

\textbf{$\epsilon$-solution:} an $x \in \R^n$ such that
\begin{align*}
    ||x||_2 &\leq R,\\
    A_0+\sum_{j=1}^n x_jA_j &\succeq -\epsilon I,\\
    c^Tx &\leq \text{Opt}(p)+\epsilon
\end{align*}

\textbf{Newton complexity of $\epsilon$-solution:}
\[\text{Compl}^\text{Nwt}(p\epsilon) = O(1)\left(1+\sum_{i=1}^m k_i\right)^{1/2}\text{Digits}(p,\epsilon).\]

\textbf{Arithmetic complexity of $\epsilon$-solution:}
\[\text{Compl}(p,\epsilon) = O(1)\left(1+\sum_{i=1}^m k_i\right)^{1/2}n\left(n^2+n\sum_{i=0}^m k_i^2 +\sum_{i=0}^m k_i^3\right)\text{Digits}(p,\epsilon).\]

\bibliographystyle{plainurl} 
\bibliography{references}
\nocite{*}

\end{document}
